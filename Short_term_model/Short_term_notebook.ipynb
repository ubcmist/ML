{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is a framework to get biometric data, process data, create & train models.\n",
    "Inspired from work of https://www.kaggle.com/coni57/model-from-arxiv-1805-00794\n",
    "'''\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.signal import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# to make model making reproducible for comparisons\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data addresses and folders\n",
    "Fitbit_m_dir = \"Data/Raw/Heart/Heart_Rate_Data/Fitbit_m/\"\n",
    "Fitbit_h_dir = \"Data/Raw/Heart/Heart_Rate_Data/Fitbit_h/\"\n",
    "\n",
    "Selected_fitbit_dir = Fitbit_m_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20181217_x.csv', '20181218_x.csv', '20181219_x.csv', '20181220_x.csv', '20181221_x.csv', '20181222_x.csv', '20181223_x.csv', '20181224_x.csv', '20181226_x.csv']\n",
      "['20181217_y.csv', '20181218_y.csv', '20181219_y.csv', '20181220_y.csv', '20181221_y.csv', '20181222_y.csv', '20181223_y.csv', '20181224_y.csv', '20181226_y.csv']\n"
     ]
    }
   ],
   "source": [
    "# region scanning the data folders to collect file addresses for input and output\n",
    "input_files_list = []\n",
    "output_files_list = []\n",
    "for root, dirs, files in os.walk(Selected_fitbit_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"_x.csv\"):\n",
    "            input_files_list += [file]\n",
    "        elif file.endswith(\"_y.csv\"):\n",
    "            output_files_list += [file]\n",
    "if len(input_files_list) != len(output_files_list):\n",
    "    raise(\"we have ODD number of files in the folder. It should be EVEN to have both inputs and outputs.\")\n",
    "print(input_files_list)\n",
    "print(output_files_list)\n",
    "# endregion scanning the data folders to collect file addresses for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_files_list)):\n",
    "    # region reading data as dataframes.\n",
    "    input_file_address = os.path.join(Selected_fitbit_dir, input_files_list[i])\n",
    "    output_file_address = os.path.join(Selected_fitbit_dir, output_files_list[i])\n",
    "    if (input_file_address[:-6] != output_file_address[:-6]):\n",
    "        raise(\"Wrong pair of csv data files selected\")\n",
    "\n",
    "    df_x = pd.read_csv(input_file_address)\n",
    "    df_y = pd.read_csv(output_file_address)\n",
    "    df_y = df_y[df_y.Anxiety_Level.isnull() == False] #removing the empty rows (rows without Anxiety_Level label)\n",
    "    # endregion reading data as dataframes.\n",
    "    \n",
    "    # region Parsing Time\n",
    "    df_x['Time'] = pd.to_datetime(df_x.Time,format= '%H:%M:%S').dt.time\n",
    "    df_y['Time'] = pd.to_datetime(df_y.Time,format= '%H:%M').dt.time\n",
    "    # endregion Parsing Time\n",
    "    \n",
    "    \n",
    "\n",
    "#     for time_y in df_y.Time:\n",
    "#         df_x_before_time_y = df_x.loc[df_x.Time < time_y]\n",
    "#         if len(df_x_before_time_y) == 0:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x[df_x.Time < df_y.Time[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:05\n",
      "0 days 00:05:00\n"
     ]
    }
   ],
   "source": [
    "period = pd.to_timedelta(5,unit='s')\n",
    "print(period)\n",
    "time_range = pd.to_timedelta(5,unit='m')\n",
    "# time_range = pd.Timedelta(seconds=300)\n",
    "print(time_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
